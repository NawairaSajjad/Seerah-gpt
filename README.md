

# üìñ Seerat-GPT

### *An AI-Powered Assistant for Learning from the Seerat-un-Nabi Ô∑∫*

Seerat-GPT is a **Retrieval-Augmented Generation (RAG)** based question-answering application designed to provide authentic, context-aware responses about the **life and teachings of Prophet Muhammad Ô∑∫ (Seerat-un-Nabi)**.
It combines the power of **local LLMs**, **semantic search**, and **Streamlit UI** to deliver meaningful insights from Islamic texts.

---

## üåü Features

‚úÖ **Ask any question about Seerat-un-Nabi Ô∑∫** and get a precise answer.
‚úÖ **Context-aware answers** retrieved from pre-embedded authentic text chunks.
‚úÖ **RAG-based approach** ‚Äî ensures responses are grounded in provided sources.
‚úÖ **Runs fully offline** using locally hosted LLM (Llama 3.2 + BGE-M3).
‚úÖ **Clean and interactive UI** built with Streamlit.

---

## üß† Technology Stack

| Layer                 | Technology                                      | Description                                                         |
| :-------------------- | :---------------------------------------------- | :------------------------------------------------------------------ |
| **Frontend/UI**       | [Streamlit](https://streamlit.io/)              | For building the web interface and user interaction.                |
| **Embeddings Model**  | [BGE-M3](https://huggingface.co/BAAI/bge-m3)    | Used for creating semantic embeddings of text chunks.               |
| **Language Model**    | [Llama 3.2](https://ollama.ai/library/llama3.2) | Generates final responses using retrieved context.                  |
| **Vector Similarity** | `scikit-learn` (Cosine Similarity)              | Measures how close user query embeddings are to stored chunks.      |
| **Data Handling**     | `pandas`, `numpy`, `joblib`                     | Manages, stores, and retrieves pre-computed embeddings efficiently. |
| **Backend API**       | [Ollama](https://ollama.ai/)                    | Serves local LLM and embedding models.                              |

---

## ‚öôÔ∏è How It Works (RAG Pipeline)

1. **User Query Input:**
   The user asks a question related to Seerat-un-Nabi Ô∑∫.

2. **Embedding Generation:**
   The query is converted into an embedding using the **BGE-M3** model.

3. **Semantic Retrieval:**
   Using **cosine similarity**, the system finds the most relevant chunks from the stored dataset (loaded via `embeddings.joblib`).

4. **Context Construction:**
   The top retrieved chunks are combined to form a **context** for the LLM.

5. **LLM Response Generation:**
   The context and query are passed to **Llama 3.2**, which generates a meaningful answer grounded in the retrieved information.

6. **Display on Streamlit UI:**
   The response is displayed neatly in the app interface.

---

## üñ•Ô∏è Installation & Setup

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/NawairaSajjad/seerah-gpt.git
cd seerah-gpt
```

### 2Ô∏è‚É£ Install dependencies

```bash
pip install streamlit requests numpy pandas scikit-learn joblib
```

### 3Ô∏è‚É£ Run Ollama locally

Make sure Ollama is installed and the required models are pulled:

```bash
ollama pull llama3.2
ollama pull bge-m3
```

### 4Ô∏è‚É£ Launch the Streamlit app

```bash
streamlit run seerah-gpt.py
```

## üåô Vision

To create an **AI assistant grounded in Islamic knowledge**, enabling users to explore and understand the **Seerah of Prophet Muhammad Ô∑∫** in an accurate, respectful, and technology-driven way.

---

## ü§ù Contributing

Contributions are welcome! You can:

* Add new datasets of Islamic texts.
* Improve UI/UX.
* Enhance retrieval and ranking logic.

---

## üïå Acknowledgment

Special thanks to open-source Islamic resources and the developers of **Ollama**, **BGE-M3**, and **Llama 3.2**, whose tools made this project possible.

